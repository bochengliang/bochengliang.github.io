---
layout:     post   				    # 使用的布局（不需要改）
title:      抓取代理并设置			# 标题 
subtitle:   欢迎来到我的博客 #副标题
date:       2018-08-20 				# 时间
author:     BY liangpaopao				# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags: python								#标签
    - 生活
---

## Hey
>学习设置代理了

 


        import requests
        import time
        from lxml import etree
        from bs4 import BeautifulSoup
        import random
        #session = requests.Session()
        Url ='https://maoyan.com/board/4
        # 爬西刺代理号
        def get_ip_list():
            headers = {
                'Host': 'www.xicidaili.com',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75     Safari/537.36'
            }
            print("正在获取代理列表...")
            url = 'http://www.xicidaili.com/nn/'
            html = requests.get(url=url, headers=headers).text
            soup = BeautifulSoup(html, 'lxml')
            ips = soup.find(id='ip_list').find_all('tr')
            ip_list = []
            for i in range(1, len(ips)):
                ip_info = ips[i]
                tds = ip_info.find_all('td')
                ip_list.append(tds[1].text + ':' + tds[2].text)
            print("代理列表抓取成功.")
            # print(ip_list)
            return ip_list
# 产生随机有效代理
        def get_random_ip(ip_list):
            print("正在设置随机代理...")
            proxy_list = []
            for ip in ip_list:
                proxy_list.append('http://' + ip)
            Feasible_Ip = Test_ifsuccess_Ip(proxy_list) #调用检测函数
            print(len(Feasible_Ip)) #测试有多少个 有效的代理
            proxy_ip = random.choice(Feasible_Ip)
            proxies = {'http': proxy_ip}  #随机设置一个有效的代理
            print("代理设置成功.")
            return proxies  
# 测试代理是否有效
        def Test_ifsuccess_Ip(proxy_list):
            Feasible_Ip = []
            lens = len(proxy_list)
            try:
                for i in range(lens):
                    print("正在测试-------")
                    res = requests.get('http://www.baidu.com', proxies={"http": proxy_list[i]})
                    print(res)
                    #测试，无效的代理 无法访问网站
                    # time.sleep(3)
                    if res.status_code == 200:
                        print("正在暴力查找有效得IP，请稍等哈--------")
                        Feasible_Ip.append([proxy_list[i]])#如果有效，那么就添加到Feasible_Ip(可行的IP队列中)
                        print("已经帮您添加代理:{}".format(proxy_list[i]))
                    else:
                        # time.sleep(3)
                        continue #如果没找到响应出错，重新选
                return Feasible_Ip
            except:
                print("出错了")
                # return None
                return Feasible_Ip
            # return Feasible_Ip
# 获取主页内容
        def get_one_page(url,proxie) :
            try:
                headers = {
                    'Host': 'maoyan.com',
                    'Referer': 'https://maoyan.com/board/4',
                    'User - Agent': 'Mozilla / 5.0(Windows NT 10.0;WOW64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 68.0.3440.75Safari / 537.36'
                }
                response = requests.get(url, headers=headers, timeout=10, proxies=proxie)
                # print(response.status_code)
                if response.status_code == 200:
                    return response.text
            except:
                return None

